[
  {
    "objectID": "posts/mac-mini-headless/index.html",
    "href": "posts/mac-mini-headless/index.html",
    "title": "Mac Mini as a headless server",
    "section": "",
    "text": "This story is probably not interesting at all and too naive. You should definitely spend your time on something better than reading it but, I decided that I would like to write more in 2023 and so far I already failed for 14 days. So I decided to write it anyway.\nI have a M1 Mac Mini that I use as a self-hosted runner for Github Actions in the torch repository and other repositories in the mlverse organisation that require specific testing for M1 macs, as currently there’s no M1 runners available by default on GitHub.\nThis Mac Mini used to stay in my office in São Paulo, and once I configured it as the CI self hosted runner I never had to connect it again to my monitor, keyboard or mouse to change any setting. Everything was up and running with no problems.\nIn the beginning of this year I temporarily moved from São Paulo to Europe, and I decided to bring the M1 Mac Mini. What if for some reason the internet was down in my office in São Paulo, or a power outage could turn the mini off and there would be no one to turn it back on.\nBack then, I didn’t check but, I thought if would be surely possible to power on the MacMini and connect it to WiFi without a monitor, keyboard and mouse. Perhaps ssh’ing from my Mac, or using a cabel for screen sharing.\nArriving at the first apartment I was going to stay, I powered it on, and started searching online how to remotely connect it to internet. The first thing I found is that since that MacMini had FileVault enabled - it wouldn’t be possible to boot it remotely. It seems that when you boot a FileVault enabled Mac, only a small portion of the OS is actually enabled so you can type your password, but that’s not enough for Remote Screen Sharing or ssh’ing into the mini.\nOK, I was probably naive. So I ran to a local store and bought a mouse. keyboard in that store were specially expensive (20 euros - if you don’t find it expensive, that’s because you are not a Brazilian making currency conversions all the time), and I didn’t want to buy a keyboard that I was going to use only to login into the mac mini. There was a TV in the apartment, I was planing to connect the HDMI cable to the mini, connect the mouse and access some kind of virtual keyboard to type my password and make it boot.\nTo my surprise, enabling the virtual keyboard is not something that you can do from the login menu, unless you have already enabled it the in the accessibility settings in the MacOS settings app. I can change the input language to whatever I want, but impossible to get the virtual keyboard to work.\nThe next day, I moved to another apartment, this one I was going to stay 2 weeks. I was convinced that I would need to buy a keyboard but, the new apartment doesn’t have a TV or monitor, thus probably not worth buying the keyboard…\nI’m moving again the next week and there will be a TV in the new apartment and I’ll probably end up buying a keyboard and everything is going to be solved. But really, it feels very weird needing a monitor, keyboard and mouse in order to be able to use this nice and small computer as a headless server."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog posts",
    "section": "",
    "text": "Autoscaling self-hosted runners for GH Actions\n\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2023\n\n\nDaniel Falbel\n\n\n\n\n\n\n\n\nMac Mini as a headless server\n\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nDaniel Falbel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/autorunner/index.html",
    "href": "posts/autorunner/index.html",
    "title": "Autoscaling self-hosted runners for GH Actions",
    "section": "",
    "text": "This blogpost describes a service called ‘autorunner’ that’s now used in the mlverse organization to auto-scale self-hosted runners for GitHub actions.\nIn mlverse we build R packages that connect to the deep learning ecosystem. GPU’s are used a lot for deep elarning and thus we need to test our packages in systems equipped with GPU’s, with multiple versions and etc.\nI used to have a linux box in my office that is equipped with a GPU, but I recently moved to another country and couldn’t bring it with me. THe machine has been disconnected from internet and can no longer serve for this.\nWe were missing GPU machines for our CI and a few alternatives were possible.\nThe problem of the approach 1 is cost. It’s quite expensive to rent a GPU machine 24h a day 7/7 for a month. And we would probably only use it for a few hours in the day. (Each CI job takes around 20 min) and we don’t need to run the GPU’s in every PR.\nWe decided to use the second approach. And code is available in the autorunner repository."
  },
  {
    "objectID": "posts/autorunner/index.html#how-it-works",
    "href": "posts/autorunner/index.html#how-it-works",
    "title": "Autoscaling self-hosted runners for GH Actions",
    "section": "How it works",
    "text": "How it works\nThe system is based on GitHub Webhooks as suggested in the self-hosted runners documentation from GitHub actions.\nIt works like this:\n\nA webhook callback is registered to listen to the workflow jobs hook. This hook is called whenever a job is queued, changes it’s status to ‘in-progress’ and when ‘completed’ and passes a lot of information to the callback endpoint, including the runner labels, repository, and etc.\nWhen the hook is called with a ‘queue’ event, it adds a task to Google Cloud Tasks queue that will launch an instance on GCE equipped with a GPU. We use a startup script to install the GitHub actions runner agent and make the agent run in ephemeral mode.\nWhen the hook is called with a ‘completed’ event, it adds a task to Google Cloud Tasks queue that will delete the instance from GCE."
  },
  {
    "objectID": "posts/autorunner/index.html#task-scheduler",
    "href": "posts/autorunner/index.html#task-scheduler",
    "title": "Autoscaling self-hosted runners for GH Actions",
    "section": "Task scheduler",
    "text": "Task scheduler\nThe service responsible to listen to the GitHub webhooks events is called task scheduler. We could make it so it creates the instance right there before returning the webhook response. However, it must respond within less then 10s which is timeout defined by GitHub and this is not enough time to launch an instance on GCE.\nBecause of this reason, this service will only create a http request task on Google Cloud Tasks.\nThis service is a plumber API hosted in Google Cloud Run.\nIt has a single endpoint that takes the webhook resquest and decides if it should register the ‘create VM instance’ or ‘delete VM instance’ task. The code is just:\nfunction(req) {\n  body <- jsonlite::fromJSON(req$postBody)\n\n  if (!\"self-hosted\" %in% body$workflow_job$labels)\n    return(\"ok\")\n\n  if (body$action == \"queued\") {\n\n    if (!\"gce\" %in% body$workflow_job$labels)\n      return(\"ok\")\n\n    instance_id <- paste0(\"ghgce-\", body$workflow_job$id, \"-\",  body$workflow_job$run_id)\n    # add some more randomstuff to the instance name to avoid collisions.\n    instance_id <- paste0(instance_id, \"-\", paste0(sample(letters, 10, replace=TRUE), collapse = \"\"))\n\n    gpu <- as.numeric(\"gpu\" %in% body$workflow_job$labels)\n    labels <- paste(body$workflow_job$labels[-1], collapse = \",\")\n    return(tasks_create_vm(instance_id, labels, gpu))\n  }\n\n  if (body$action == \"completed\") {\n    instance_id <- as.character(body$workflow_job$runner_name)\n\n    if (is.null(instance_id)) {\n      return(\"nothing to delete\")\n    }\n\n    if (!grepl(\"ghgce\", instance_id)) {\n      return(\"not gce allocated instance\")\n    }\n\n    return(tasks_delete_vm(instance_id))\n  }\n\n  return(\"nothing to do\")\n}\nAs you can see we use the workflow_job$labels field in the webhook payload to decide the instance type (GPU or not) and also whether we should really create an instance. For deleting the instance we can use the workflow_job$runner_name as it will contain the GCE instance name.\nThe tasks are registered with eg. tasks_create_vm that just makes a call to the Cloud Tasks API. Using gargle for authentication.\nRegistering a task to create a VM executes something like:\ntasks_create_vm <- function(instance_id, labels, gpu) {\n  cloud_tasks_request(\n    method = \"POST\",\n    body = list(\n      task = list(\n        httpRequest = list(\n          url = paste0(Sys.getenv(\"SERVICE_URL\"), \"vm_create\"),\n          httpMethod = \"POST\",\n          body = openssl::base64_encode(jsonlite::toJSON(auto_unbox = TRUE, list(\n            instance_id = instance_id,\n            labels = labels,\n            gpu = gpu\n          )))\n        )\n      )\n    )\n  )\n}\nI’d like to note the url here in the httpRequest item of the body. url = paste0(Sys.getenv(\"SERVICE_URL\"), \"vm_create\") will be the URL and endpoint name for the task handler service that we describe below. Cloud Tasks will be responsible to making the defined http request on time, and handle failures, retries, etc."
  },
  {
    "objectID": "posts/autorunner/index.html#task-handler",
    "href": "posts/autorunner/index.html#task-handler",
    "title": "Autoscaling self-hosted runners for GH Actions",
    "section": "Task handler",
    "text": "Task handler\nThe task handler is responsible for executing the VM creation and deletion tasks. It’s again a plumber API hosted on Cloud Run with endpoints for each VM task (create and delete).\nHere’s an example of the endpoint that creates the VM using the googleComputeEngineR package. The VM creation is pretty standard except for the startup script that will install the GH Actions runner agent and other addition software - including the GPU drivers when a GPU instance is requested.\nThe endpoint is defined as:\n#* Start a new runner with specified options\n#*\n#* @post vm_create\nfunction(instance_id, labels, gpu) {\n  gpu <- as.numeric(gpu)\n  start_gce_vm(instance_id, labels, gpu)\n}\nAnd the VM is started with:\nstartup_script <- function(org, labels, gpu) {\n  token <- gh::gh(\"POST /orgs/{org}/actions/runners/registration-token\", org = org)\n  glue::glue(\n    readr::read_file(\"bootstrap.sh\"),\n    org = org,\n    runner_token = token$token,\n    labels = labels\n  )\n}\n\nstart_gce_vm <- function(instance_id, labels, gpu) {\n  googleComputeEngineR::gce_vm(\n    instance_id,\n    image_project = \"ubuntu-os-cloud\",\n    image_family = \"ubuntu-2204-lts\",\n    predefined_type = \"n1-standard-4\",\n    disk_size_gb = 90,\n    project = googleComputeEngineR::gce_get_global_project(),\n    zone = googleComputeEngineR::gce_get_global_zone(),\n    metadata = list(\n      \"startup-script\" = startup_script(\n        org = \"mlverse\",\n        labels = labels,\n        gpu = gpu\n      )\n    ),\n    acceleratorCount = if (gpu) 1 else NULL,\n    acceleratorType = if (gpu) \"nvidia-tesla-t4\" else \"\",\n    scheduling = list(\n      'preemptible' = TRUE\n    )\n  )\n}\nWe use preemptible VM’s to further reduce costs. This is defined with scheduling = list('preemptible' = TRUE) in the above function.\nNote the metadata argument in googleComputeEngineR::gce_vm. Here we pass a shell script that installs the required software. This script is modified based the kind of VM that is created, the labels that you want to the self-hosted runner once it’s registered and a GH Actions token, used to allow registering the runner into GH.\nHere’s our script content:\n#! /bin/bash\n\nadduser --disabled-password --gecos \"\" actions\ncd /home/actions\n\n# set self-destructing after 1 hour\n# this will turn off the instance, but won't delete its disk, etc.\n# at least can avoid some costs.\nsudo shutdown -h +90\n\n# install docker\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n\nif [ \"{gpu}\" == \"1\" ]\nthen\n  # GPU driver installation instructions from:\n  # https://cloud.google.com/compute/docs/gpus/install-drivers-gpu\n  curl https://autorunner-task-handler-fzwjxdcwoq-uc.a.run.app/driver/install_gpu_driver.py --output install_gpu_driver.py\n  sudo python3 install_gpu_driver.py\n\n\n  distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\\n      && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n      && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \\\n            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n  sudo apt-get update\n  sudo apt-get install -y nvidia-docker2\n  sudo systemctl restart docker\nfi\n\nsudo -u actions mkdir actions-runner && cd actions-runner\nsudo -u actions curl -o actions-runner-linux-x64-2.300.2.tar.gz -L https://github.com/actions/runner/releases/download/v2.300.2/actions-runner-linux-x64-2.300.2.tar.gz\nsudo -u actions echo \"ed5bf2799c1ef7b2dd607df66e6b676dff8c44fb359c6fedc9ebf7db53339f0c  actions-runner-linux-x64-2.300.2.tar.gz\" | shasum -a 256 -c\nsudo -u actions tar xzf ./actions-runner-linux-x64-2.300.2.tar.gz\nsudo -u actions ./config.sh --url https://github.com/{org} --token {runner_token} --ephemeral --labels {labels} --unattended\n./svc.sh install\n./svc.sh start\nThe installation of GPU drivers is handled by a script copied and slightly modified to install more recent drivers from Google Cloud documentation. We also install docker and nvidia-docker to be able to execute GH action jobs with a container specification. Specially for CUDA jobs it’s much easier to use containers than to install CUDA/CUDNN. That’s it. Feel free to copy and modify scripts in the autorunner repository."
  },
  {
    "objectID": "posts/autorunner/index.html#future-improvements",
    "href": "posts/autorunner/index.html#future-improvements",
    "title": "Autoscaling self-hosted runners for GH Actions",
    "section": "Future improvements",
    "text": "Future improvements\nThere are many future improvements that we could provide in the future:\n\nAllow specifying the GCE machine type using labels, eg a workflow with runner: [self-hosted, gce, n1-standard-4] would start a n1-standard-4 machine. It would also be nice to allow other kinds of configurations like the disk size, image template, etc.\nOne flaw with this process is that sometimes a registered runner is picked up by another job in the organization that uses the same labels and is queued. There should be a way to chack if the actual job started after the VM has been registered and if not, try creating a new VM.\nInstead of always installing the GPU drivers we could create VM images that already have it installed so we don’t loose about ~5min of execution just installing the GPU drivers over and over. I’d expect Google or NVIDIA should provide images with the drivers pre-installed instead though.\nWindows support. GCE allows booting Windows VM instances. We would need to figure out how to automatically install the GitHub Actions agent and the CUDA drivers."
  }
]