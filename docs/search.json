[
  {
    "objectID": "posts/reticulate-nas/index.html",
    "href": "posts/reticulate-nas/index.html",
    "title": "Reticulate handling of NA’s",
    "section": "",
    "text": "There’s room for improving how reticulate handles missing values when converting between R and Python (pandas) data.frames. This documents highlights the current behavior and takes inspiration on Pandas <-> Arrow casts to propose improvements."
  },
  {
    "objectID": "posts/reticulate-nas/index.html#current-behavior",
    "href": "posts/reticulate-nas/index.html#current-behavior",
    "title": "Reticulate handling of NA’s",
    "section": "Current behavior",
    "text": "Current behavior\nThe following data.frame contains NA’s in the 4 most used data types in R. We use tibble::as_tibble() to show the data.frame as it better displays the data types.\n\ndf <- data.frame(\n  bool = c(NA, TRUE, FALSE),\n  int = c(NA, 1L, 2L),\n  num = c(NA, 0.1, 0.2),\n  char = c(NA, \"a\", \"b\")\n)\ntibble::as_tibble(df)\n\n# A tibble: 3 × 4\n  bool    int   num char \n  <lgl> <int> <dbl> <chr>\n1 NA       NA  NA   <NA> \n2 TRUE      1   0.1 a    \n3 FALSE     2   0.2 b    \n\n\nNow casting into a pandas data frame. We see the following:\n\np_df <- reticulate::r_to_py(df)\n\n\nFor bool: NA became TRUE which is quite unexpected.\nFor int: NA became the largest possible integer. This is also unexpected. However pandas default integer data type does not support NA’s and in theory one must cast them to float.\nFor num: we got a NaN, which is the default missing value in Pandas, even though an experimental pd.NA value.\nFor char, we get NA as a character, which is also very unlikely to be the best way to handle the conversion.\n\n\nGetting values back into R\nCasting back the pandas data.frame into R, we get:\n\nreticulate::py_to_r(p_df)\n\n   bool int num char\n1  TRUE  NA  NA   NA\n2  TRUE   1 0.1    a\n3 FALSE   2 0.2    b\n\n\nIt mostly works, besides that the missing boolean value is lost."
  },
  {
    "objectID": "posts/reticulate-nas/index.html#how-others-do-it",
    "href": "posts/reticulate-nas/index.html#how-others-do-it",
    "title": "Reticulate handling of NA’s",
    "section": "How others do it",
    "text": "How others do it\nreticulate is not the only library that needs to convert tables with such types that happen to contain missing values into pandas data frames. We looked into how Arrow and Polars work in such cases. Both libraries support missing values via an explicit NULL value. Polars is based on Arrow, so there shouldn’t exist many differences compared to Arrow besides some additional handling of NaNs.\n\nimport pyarrow as pa\n\nb = pa.array([None, True, False])\ni = pa.array([None, 1, 2])\nn = pa.array([None, 0.1, 0.2])\nc = pa.array([None, \"a\", \"b\"])\n\nat = pa.table([b, i, n, c], names=[\"bool\", \"int\", \"num\", \"char\"])\n\nAnd this is the result of the cast:\n\np_df = at.to_pandas()\np_df\n\n    bool  int  num  char\n0   None  NaN  NaN  None\n1   True  1.0  0.1     a\n2  False  2.0  0.2     b\n\np_df.dtypes\n\nbool     object\nint     float64\nnum     float64\nchar     object\ndtype: object\n\n\nNote that:\n\nbool and char were cast into object types. The object data type in Pandas is used for columns with mixed types. One possible downside of this approach is that NAs become bool after any comparison. It also cast to False (or 0) when you sum a column containing a None.\n\np_df['bool'] & True\n\n0    False\n1     True\n2    False\nName: bool, dtype: bool\n\np_df['bool'] | True\n\n0    False\n1     True\n2     True\nName: bool, dtype: bool\n\np_df['bool'].sum()\n\n1\n\np_df['bool'].isna()\n\n0     True\n1    False\n2    False\nName: bool, dtype: bool\n\n\nint has been cast into a ‘float64’ type. This reflects Pandas default approach too, since integer values can’t represent NaNs (the default missing value). This approach seems reasonable for reticulate to consider - specially considering how R is flexible in general with numerics and integers.\nnum: the default missing value (NaN) is used.\n\n\nWhat happens with round trip casts\nIt seems that from the above it’s hard to get back the same arrow table that was first converted. Let’s try:\n\npa.Table.from_pandas(p_df)\n\npyarrow.Table\nbool: bool\nint: double\nnum: double\nchar: string\n----\nbool: [[null,true,false]]\nint: [[null,1,2]]\nnum: [[null,0.1,0.2]]\nchar: [[null,\"a\",\"b\"]]\n\n\nThis works quite fine. The only information that has been lost is the data type of the integer column, that has been transformed into a float.\nTODO: figure out how arrow does this. Does it walk trough pandas object columns, and if it’s constant besides the NULL, it uses that data type?\n\n\nUsing Pandas nullable data types\nArrow supports using Pandas nullable data types with:\n\nimport pandas as pd\ndtype_mapping = {\n    pa.int8(): pd.Int8Dtype(),\n    pa.int16(): pd.Int16Dtype(),\n    pa.int32(): pd.Int32Dtype(),\n    pa.int64(): pd.Int64Dtype(),\n    pa.uint8(): pd.UInt8Dtype(),\n    pa.uint16(): pd.UInt16Dtype(),\n    pa.uint32(): pd.UInt32Dtype(),\n    pa.uint64(): pd.UInt64Dtype(),\n    pa.bool_(): pd.BooleanDtype(),\n    pa.float32(): pd.Float32Dtype(),\n    pa.float64(): pd.Float64Dtype(),\n    pa.string(): pd.StringDtype(),\n}\n\np_df_nullable = at.to_pandas(types_mapper=dtype_mapping.get)\n\nIN such cases, the round trip cast also works perfectly:\n\npa.Table.from_pandas(p_df_nullable)\n\npyarrow.Table\nbool: bool\nint: int64\nnum: double\nchar: string\n----\nbool: [[null,true,false]]\nint: [[null,1,2]]\nnum: [[null,0.1,0.2]]\nchar: [[null,\"a\",\"b\"]]\n\n\n\n\nArrow -> Pandas -> R?\nOne question that came up is what happens if we take the Arrow table (that natively) containing missing values, cast into pandas and then into R. Can reticulate correctly infer data types?\n\ntibble::as_tibble(reticulate::py$p_df)\n\n# A tibble: 3 × 4\n  bool        int   num char     \n  <list>    <dbl> <dbl> <list>   \n1 <NULL>      NaN NaN   <NULL>   \n2 <lgl [1]>     1   0.1 <chr [1]>\n3 <lgl [1]>     2   0.2 <chr [1]>\n\n\nIt seems reasonable, but we don’t simplify the columns types which Arrow does nicely.\nWhat if we get the Pandas table that uses the nullable data types:\n\nreticulate::py$p_df_nullable\n\nWarning in format.data.frame(if (omit) x[seq_len(n0), , drop = FALSE] else x, :\ncorrupt data frame: columns will be truncated or padded with NAs\n\n\n                                                            bool\n1 <BooleanArray>\\n[<NA>, True, False]\\nLength: 3, dtype: boolean\n2                                                           <NA>\n3                                                           <NA>\n                                                    int\n1 <IntegerArray>\\n[<NA>, 1, 2]\\nLength: 3, dtype: Int64\n2                                                  <NA>\n3                                                  <NA>\n                                                           num\n1 <FloatingArray>\\n[<NA>, 0.1, 0.2]\\nLength: 3, dtype: Float64\n2                                                         <NA>\n3                                                         <NA>\n                                                       char\n1 <StringArray>\\n[<NA>, 'a', 'b']\\nLength: 3, dtype: string\n2                                                      <NA>\n3                                                      <NA>\n\n\nDoesn’t work.\n\n\nPolars\nWe don’t expect many differences in behavior between Arrow and Polars, so we just quickly print the conversion results:\n\nimport polars as pl\npl_df = pl.DataFrame({\n  'bool': [None, True, False],\n  'int':  [None, 1, 2],\n  'num':  [None, 0.1, 0.2],\n  'char': [None, \"a\", \"b\"],\n})\nprint(pl_df)\n\nshape: (3, 4)\n┌───────┬──────┬──────┬──────┐\n│ bool  ┆ int  ┆ num  ┆ char │\n│ ---   ┆ ---  ┆ ---  ┆ ---  │\n│ bool  ┆ i64  ┆ f64  ┆ str  │\n╞═══════╪══════╪══════╪══════╡\n│ null  ┆ null ┆ null ┆ null │\n│ true  ┆ 1    ┆ 0.1  ┆ a    │\n│ false ┆ 2    ┆ 0.2  ┆ b    │\n└───────┴──────┴──────┴──────┘\n\n\nConverting into pandas\n\np_df = pl_df.to_pandas()\np_df\n\n    bool  int  num  char\n0   None  NaN  NaN  None\n1   True  1.0  0.1     a\n2  False  2.0  0.2     b\n\np_df.dtypes\n\nbool     object\nint     float64\nnum     float64\nchar     object\ndtype: object\n\n\nAnd getting back into polars:\n\nprint(pl.DataFrame(p_df))\n\nshape: (3, 4)\n┌───────┬──────┬──────┬──────┐\n│ bool  ┆ int  ┆ num  ┆ char │\n│ ---   ┆ ---  ┆ ---  ┆ ---  │\n│ bool  ┆ f64  ┆ f64  ┆ str  │\n╞═══════╪══════╪══════╪══════╡\n│ null  ┆ null ┆ null ┆ null │\n│ true  ┆ 1.0  ┆ 0.1  ┆ a    │\n│ false ┆ 2.0  ┆ 0.2  ┆ b    │\n└───────┴──────┴──────┴──────┘\n\n\nSame as with Arrow.\n\n\nPySpark\nWe also looked at how Spark casts its DataFrames, that supports nullable data types into Pandas data frames and back.\n\nimport findspark\nfindspark.init()\nimport pyspark as ps\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\ns_df = spark.createDataFrame([\n  (None, None, None, None),\n  (True, 1, 0.1, \"a\"),\n  (False, 2, 0.2, \"b\")], \n  [\"bool\", \"int\", \"num\", \"char\"]\n  )\ns_df\n\n\n\ns_df.head(3)\n\n[Row(bool=None, int=None, num=None, char=None), Row(bool=True, int=1, num=0.1, char='a'), Row(bool=False, int=2, num=0.2, char='b')]\n\n\nNow collect into a pandas data frame:\n\np_df = s_df.toPandas()\np_df\n\n    bool  int  num  char\n0   None  NaN  NaN  None\n1   True  1.0  0.1     a\n2  False  2.0  0.2     b\n\np_df.dtypes\n\nbool     object\nint     float64\nnum     float64\nchar     object\ndtype: object\n\n\nIt looks like PySpark is using the same behavior as Arrow (and Polars) which is to use objects for booleans and chars, representing the null with Python None. Integers are converted into floats and then use NaN for representing the missing value.\nNow going back to spark:\n\ns_df = spark.createDataFrame(p_df)\ns_df\n\n\n\ns_df.head(3)\n\n[Row(bool=None, int=nan, num=nan, char=None), Row(bool=True, int=1.0, num=0.1, char='a'), Row(bool=False, int=2.0, num=0.2, char='b')]\n\n\nWhen going back, the types are simplifed (object -> boolean, object -> string). Differently from Arrow, NaN are kept and not converted into None or the Spark null value.\nCan we use the Pandas’s nullable datatypes when casting from Spark? I couldn’t find it, although you can:\n\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\nspark.createDataFrame(s_df.toPandas()).head(3)\n\n[Row(bool=None, int=None, num=None, char=None), Row(bool=True, int=1.0, num=0.1, char='a'), Row(bool=False, int=2.0, num=0.2, char='b')]\n\n\nAnd now, you get None instead of NaNs to represent missing values for integers and numeric. Although this is tricky because it would wrongly convert real NaNs. into nulls.\nBy setting this feature to True, you also get the casts from nullable pandas data types:\n\nspark.createDataFrame(p_df_nullable).head(3)\n\n[Row(bool=None, int=None, num=None, char=None), Row(bool=True, int=1, num=0.1, char='a'), Row(bool=False, int=2, num=0.2, char='b')]"
  },
  {
    "objectID": "posts/reticulate-nas/index.html#future-actions",
    "href": "posts/reticulate-nas/index.html#future-actions",
    "title": "Reticulate handling of NA’s",
    "section": "Future actions",
    "text": "Future actions\nGiven this analysis, I think it make sense to make the following changes to reticulate:\n\nWe should provide an option to use Pandas nullable data types when casting from R to Pandas. And maybe - maybe - this should be the default. Given the current behavior, it seems that this is much safer.\nWe should support casting from Pandas nullable data types to R.\nSimilar to how Arrow and Spark works, when converting Pandas object columns to R, we should simply their data type if it only contains None and another scalar type.\n\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.11          rstudioapi_0.14      knitr_1.43          \n [4] magrittr_2.0.3       lattice_0.20-45      rlang_1.1.1         \n [7] fastmap_1.1.1        fansi_1.0.4          tools_4.2.2         \n[10] grid_4.2.2           xfun_0.39            png_0.1-8           \n[13] utf8_1.2.3           cli_3.6.1            htmltools_0.5.4     \n[16] yaml_2.3.7           digest_0.6.31        tibble_3.2.1        \n[19] lifecycle_1.0.3      Matrix_1.5-1         htmlwidgets_1.6.0   \n[22] vctrs_0.6.2          glue_1.6.2           evaluate_0.19       \n[25] rmarkdown_2.19       compiler_4.2.2       pillar_1.9.0        \n[28] reticulate_1.30-9000 jsonlite_1.8.7       pkgconfig_2.0.3     \n\n\n\nreticulate::py_config()\n\npython:         /Users/dfalbel/Documents/venv/reticulate/bin/python\nlibpython:      /Users/dfalbel/.pyenv/versions/3.9.16/lib/libpython3.9.dylib\npythonhome:     /Users/dfalbel/Documents/venv/reticulate:/Users/dfalbel/Documents/venv/reticulate\nversion:        3.9.16 (main, Dec 20 2022, 15:32:28)  [Clang 14.0.0 (clang-1400.0.29.202)]\nnumpy:          /Users/dfalbel/Documents/venv/reticulate/lib/python3.9/site-packages/numpy\nnumpy_version:  1.24.0\n\nNOTE: Python version was forced by RETICULATE_PYTHON\n\n\n\npd.show_versions()\n\n\nINSTALLED VERSIONS\n------------------\ncommit           : 8dab54d6573f7186ff0c3b6364d5e4dd635ff3e7\npython           : 3.9.16.final.0\npython-bits      : 64\nOS               : Darwin\nOS-release       : 22.5.0\nVersion          : Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:20 PDT 2023; root:xnu-8796.121.3~7/RELEASE_ARM64_T6000\nmachine          : arm64\nprocessor        : arm\nbyteorder        : little\nLC_ALL           : None\nLANG             : en_US.UTF-8\nLOCALE           : en_US.UTF-8\n\npandas           : 1.5.2\nnumpy            : 1.24.0\npytz             : 2022.7\ndateutil         : 2.8.2\nsetuptools       : 58.1.0\npip              : 22.0.4\nCython           : None\npytest           : None\nhypothesis       : None\nsphinx           : None\nblosc            : None\nfeather          : None\nxlsxwriter       : None\nlxml.etree       : None\nhtml5lib         : None\npymysql          : None\npsycopg2         : None\njinja2           : 3.1.2\nIPython          : 8.14.0\npandas_datareader: None\nbs4              : None\nbottleneck       : None\nbrotli           : None\nfastparquet      : None\nfsspec           : 2023.4.0\ngcsfs            : None\nmatplotlib       : 3.7.2\nnumba            : None\nnumexpr          : None\nodfpy            : None\nopenpyxl         : None\npandas_gbq       : None\npyarrow          : 12.0.1\npyreadstat       : None\npyxlsb           : None\ns3fs             : None\nscipy            : None\nsnappy           : None\nsqlalchemy       : None\ntables           : None\ntabulate         : 0.9.0\nxarray           : None\nxlrd             : None\nxlwt             : None\nzstandard        : None\ntzdata           : None\n\n\n\npl.show_versions()\n\n--------Version info---------\nPolars:              0.18.6\nIndex type:          UInt32\nPlatform:            macOS-13.4.1-arm64-arm-64bit\nPython:              3.9.16 (main, Dec 20 2022, 15:43:04) \n[Clang 14.0.0 (clang-1400.0.29.202)]\n\n----Optional dependencies----\nadbc_driver_sqlite:  <not installed>\nconnectorx:          <not installed>\ndeltalake:           <not installed>\nfsspec:              2023.4.0\nmatplotlib:          3.7.2\nnumpy:               1.24.0\npandas:              1.5.2\npyarrow:             12.0.1\npydantic:            <not installed>\nsqlalchemy:          <not installed>\nxlsx2csv:            <not installed>\nxlsxwriter:          <not installed>\n\n\n\nps.__version__\n\n'3.4.1'"
  },
  {
    "objectID": "posts/autorunner/index.html",
    "href": "posts/autorunner/index.html",
    "title": "Autoscaling self-hosted runners for GH Actions",
    "section": "",
    "text": "This blogpost describes a service called ‘autorunner’ that’s now used in the mlverse organization to auto-scale self-hosted runners for GitHub actions.\nIn mlverse we build R packages that connect to the deep learning ecosystem. GPU’s are used a lot for deep elarning and thus we need to test our packages in systems equipped with GPU’s, with multiple versions and etc.\nI used to have a linux box in my office that is equipped with a GPU, but I recently moved to another country and couldn’t bring it with me. THe machine has been disconnected from internet and can no longer serve for this.\nWe were missing GPU machines for our CI and a few alternatives were possible.\nThe problem of the approach 1 is cost. It’s quite expensive to rent a GPU machine 24h a day 7/7 for a month. And we would probably only use it for a few hours in the day. (Each CI job takes around 20 min) and we don’t need to run the GPU’s in every PR.\nWe decided to use the second approach. And code is available in the autorunner repository."
  },
  {
    "objectID": "posts/autorunner/index.html#how-it-works",
    "href": "posts/autorunner/index.html#how-it-works",
    "title": "Autoscaling self-hosted runners for GH Actions",
    "section": "How it works",
    "text": "How it works\nThe system is based on GitHub Webhooks as suggested in the self-hosted runners documentation from GitHub actions.\nIt works like this:\n\nA webhook callback is registered to listen to the workflow jobs hook. This hook is called whenever a job is queued, changes it’s status to ‘in-progress’ and when ‘completed’ and passes a lot of information to the callback endpoint, including the runner labels, repository, and etc.\nWhen the hook is called with a ‘queue’ event, it adds a task to Google Cloud Tasks queue that will launch an instance on GCE equipped with a GPU. We use a startup script to install the GitHub actions runner agent and make the agent run in ephemeral mode.\nWhen the hook is called with a ‘completed’ event, it adds a task to Google Cloud Tasks queue that will delete the instance from GCE."
  },
  {
    "objectID": "posts/autorunner/index.html#task-scheduler",
    "href": "posts/autorunner/index.html#task-scheduler",
    "title": "Autoscaling self-hosted runners for GH Actions",
    "section": "Task scheduler",
    "text": "Task scheduler\nThe service responsible to listen to the GitHub webhooks events is called task scheduler. We could make it so it creates the instance right there before returning the webhook response. However, it must respond within less then 10s which is timeout defined by GitHub and this is not enough time to launch an instance on GCE.\nBecause of this reason, this service will only create a http request task on Google Cloud Tasks.\nThis service is a plumber API hosted in Google Cloud Run.\nIt has a single endpoint that takes the webhook resquest and decides if it should register the ‘create VM instance’ or ‘delete VM instance’ task. The code is just:\nfunction(req) {\n  body <- jsonlite::fromJSON(req$postBody)\n\n  if (!\"self-hosted\" %in% body$workflow_job$labels)\n    return(\"ok\")\n\n  if (body$action == \"queued\") {\n\n    if (!\"gce\" %in% body$workflow_job$labels)\n      return(\"ok\")\n\n    instance_id <- paste0(\"ghgce-\", body$workflow_job$id, \"-\",  body$workflow_job$run_id)\n    # add some more randomstuff to the instance name to avoid collisions.\n    instance_id <- paste0(instance_id, \"-\", paste0(sample(letters, 10, replace=TRUE), collapse = \"\"))\n\n    gpu <- as.numeric(\"gpu\" %in% body$workflow_job$labels)\n    labels <- paste(body$workflow_job$labels[-1], collapse = \",\")\n    return(tasks_create_vm(instance_id, labels, gpu))\n  }\n\n  if (body$action == \"completed\") {\n    instance_id <- as.character(body$workflow_job$runner_name)\n\n    if (is.null(instance_id)) {\n      return(\"nothing to delete\")\n    }\n\n    if (!grepl(\"ghgce\", instance_id)) {\n      return(\"not gce allocated instance\")\n    }\n\n    return(tasks_delete_vm(instance_id))\n  }\n\n  return(\"nothing to do\")\n}\nAs you can see we use the workflow_job$labels field in the webhook payload to decide the instance type (GPU or not) and also whether we should really create an instance. For deleting the instance we can use the workflow_job$runner_name as it will contain the GCE instance name.\nThe tasks are registered with eg. tasks_create_vm that just makes a call to the Cloud Tasks API. Using gargle for authentication.\nRegistering a task to create a VM executes something like:\ntasks_create_vm <- function(instance_id, labels, gpu) {\n  cloud_tasks_request(\n    method = \"POST\",\n    body = list(\n      task = list(\n        httpRequest = list(\n          url = paste0(Sys.getenv(\"SERVICE_URL\"), \"vm_create\"),\n          httpMethod = \"POST\",\n          body = openssl::base64_encode(jsonlite::toJSON(auto_unbox = TRUE, list(\n            instance_id = instance_id,\n            labels = labels,\n            gpu = gpu\n          )))\n        )\n      )\n    )\n  )\n}\nI’d like to note the url here in the httpRequest item of the body. url = paste0(Sys.getenv(\"SERVICE_URL\"), \"vm_create\") will be the URL and endpoint name for the task handler service that we describe below. Cloud Tasks will be responsible to making the defined http request on time, and handle failures, retries, etc."
  },
  {
    "objectID": "posts/autorunner/index.html#task-handler",
    "href": "posts/autorunner/index.html#task-handler",
    "title": "Autoscaling self-hosted runners for GH Actions",
    "section": "Task handler",
    "text": "Task handler\nThe task handler is responsible for executing the VM creation and deletion tasks. It’s again a plumber API hosted on Cloud Run with endpoints for each VM task (create and delete).\nHere’s an example of the endpoint that creates the VM using the googleComputeEngineR package. The VM creation is pretty standard except for the startup script that will install the GH Actions runner agent and other addition software - including the GPU drivers when a GPU instance is requested.\nThe endpoint is defined as:\n#* Start a new runner with specified options\n#*\n#* @post vm_create\nfunction(instance_id, labels, gpu) {\n  gpu <- as.numeric(gpu)\n  start_gce_vm(instance_id, labels, gpu)\n}\nAnd the VM is started with:\nstartup_script <- function(org, labels, gpu) {\n  token <- gh::gh(\"POST /orgs/{org}/actions/runners/registration-token\", org = org)\n  glue::glue(\n    readr::read_file(\"bootstrap.sh\"),\n    org = org,\n    runner_token = token$token,\n    labels = labels\n  )\n}\n\nstart_gce_vm <- function(instance_id, labels, gpu) {\n  googleComputeEngineR::gce_vm(\n    instance_id,\n    image_project = \"ubuntu-os-cloud\",\n    image_family = \"ubuntu-2204-lts\",\n    predefined_type = \"n1-standard-4\",\n    disk_size_gb = 90,\n    project = googleComputeEngineR::gce_get_global_project(),\n    zone = googleComputeEngineR::gce_get_global_zone(),\n    metadata = list(\n      \"startup-script\" = startup_script(\n        org = \"mlverse\",\n        labels = labels,\n        gpu = gpu\n      )\n    ),\n    acceleratorCount = if (gpu) 1 else NULL,\n    acceleratorType = if (gpu) \"nvidia-tesla-t4\" else \"\",\n    scheduling = list(\n      'preemptible' = TRUE\n    )\n  )\n}\nWe use preemptible VM’s to further reduce costs. This is defined with scheduling = list('preemptible' = TRUE) in the above function.\nNote the metadata argument in googleComputeEngineR::gce_vm. Here we pass a shell script that installs the required software. This script is modified based the kind of VM that is created, the labels that you want to the self-hosted runner once it’s registered and a GH Actions token, used to allow registering the runner into GH.\nHere’s our script content:\n#! /bin/bash\n\nadduser --disabled-password --gecos \"\" actions\ncd /home/actions\n\n# set self-destructing after 1 hour\n# this will turn off the instance, but won't delete its disk, etc.\n# at least can avoid some costs.\nsudo shutdown -h +90\n\n# install docker\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n\nif [ \"{gpu}\" == \"1\" ]\nthen\n  # GPU driver installation instructions from:\n  # https://cloud.google.com/compute/docs/gpus/install-drivers-gpu\n  curl https://autorunner-task-handler-fzwjxdcwoq-uc.a.run.app/driver/install_gpu_driver.py --output install_gpu_driver.py\n  sudo python3 install_gpu_driver.py\n\n\n  distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\\n      && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n      && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \\\n            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n  sudo apt-get update\n  sudo apt-get install -y nvidia-docker2\n  sudo systemctl restart docker\nfi\n\nsudo -u actions mkdir actions-runner && cd actions-runner\nsudo -u actions curl -o actions-runner-linux-x64-2.300.2.tar.gz -L https://github.com/actions/runner/releases/download/v2.300.2/actions-runner-linux-x64-2.300.2.tar.gz\nsudo -u actions echo \"ed5bf2799c1ef7b2dd607df66e6b676dff8c44fb359c6fedc9ebf7db53339f0c  actions-runner-linux-x64-2.300.2.tar.gz\" | shasum -a 256 -c\nsudo -u actions tar xzf ./actions-runner-linux-x64-2.300.2.tar.gz\nsudo -u actions ./config.sh --url https://github.com/{org} --token {runner_token} --ephemeral --labels {labels} --unattended\n./svc.sh install\n./svc.sh start\nThe installation of GPU drivers is handled by a script copied and slightly modified to install more recent drivers from Google Cloud documentation. We also install docker and nvidia-docker to be able to execute GH action jobs with a container specification. Specially for CUDA jobs it’s much easier to use containers than to install CUDA/CUDNN. That’s it. Feel free to copy and modify scripts in the autorunner repository."
  },
  {
    "objectID": "posts/autorunner/index.html#future-improvements",
    "href": "posts/autorunner/index.html#future-improvements",
    "title": "Autoscaling self-hosted runners for GH Actions",
    "section": "Future improvements",
    "text": "Future improvements\nThere are many future improvements that we could provide in the future:\n\nAllow specifying the GCE machine type using labels, eg a workflow with runner: [self-hosted, gce, n1-standard-4] would start a n1-standard-4 machine. It would also be nice to allow other kinds of configurations like the disk size, image template, etc.\nOne flaw with this process is that sometimes a registered runner is picked up by another job in the organization that uses the same labels and is queued. There should be a way to chack if the actual job started after the VM has been registered and if not, try creating a new VM.\nInstead of always installing the GPU drivers we could create VM images that already have it installed so we don’t loose about ~5min of execution just installing the GPU drivers over and over. I’d expect Google or NVIDIA should provide images with the drivers pre-installed instead though.\nWindows support. GCE allows booting Windows VM instances. We would need to figure out how to automatically install the GitHub Actions agent and the CUDA drivers."
  },
  {
    "objectID": "posts/mac-mini-headless/index.html",
    "href": "posts/mac-mini-headless/index.html",
    "title": "Mac Mini as a headless server",
    "section": "",
    "text": "This story is probably not interesting at all and too naive. You should definitely spend your time on something better than reading it but, I decided that I would like to write more in 2023 and so far I already failed for 14 days. So I decided to write it anyway.\nI have a M1 Mac Mini that I use as a self-hosted runner for Github Actions in the torch repository and other repositories in the mlverse organisation that require specific testing for M1 macs, as currently there’s no M1 runners available by default on GitHub.\nThis Mac Mini used to stay in my office in São Paulo, and once I configured it as the CI self hosted runner I never had to connect it again to my monitor, keyboard or mouse to change any setting. Everything was up and running with no problems.\nIn the beginning of this year I temporarily moved from São Paulo to Europe, and I decided to bring the M1 Mac Mini. What if for some reason the internet was down in my office in São Paulo, or a power outage could turn the mini off and there would be no one to turn it back on.\nBack then, I didn’t check but, I thought if would be surely possible to power on the MacMini and connect it to WiFi without a monitor, keyboard and mouse. Perhaps ssh’ing from my Mac, or using a cabel for screen sharing.\nArriving at the first apartment I was going to stay, I powered it on, and started searching online how to remotely connect it to internet. The first thing I found is that since that MacMini had FileVault enabled - it wouldn’t be possible to boot it remotely. It seems that when you boot a FileVault enabled Mac, only a small portion of the OS is actually enabled so you can type your password, but that’s not enough for Remote Screen Sharing or ssh’ing into the mini.\nOK, I was probably naive. So I ran to a local store and bought a mouse. keyboard in that store were specially expensive (20 euros - if you don’t find it expensive, that’s because you are not a Brazilian making currency conversions all the time), and I didn’t want to buy a keyboard that I was going to use only to login into the mac mini. There was a TV in the apartment, I was planing to connect the HDMI cable to the mini, connect the mouse and access some kind of virtual keyboard to type my password and make it boot.\nTo my surprise, enabling the virtual keyboard is not something that you can do from the login menu, unless you have already enabled it the in the accessibility settings in the MacOS settings app. I can change the input language to whatever I want, but impossible to get the virtual keyboard to work.\nThe next day, I moved to another apartment, this one I was going to stay 2 weeks. I was convinced that I would need to buy a keyboard but, the new apartment doesn’t have a TV or monitor, thus probably not worth buying the keyboard…\nI’m moving again the next week and there will be a TV in the new apartment and I’ll probably end up buying a keyboard and everything is going to be solved. But really, it feels very weird needing a monitor, keyboard and mouse in order to be able to use this nice and small computer as a headless server."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog posts",
    "section": "",
    "text": "Reticulate handling of NA’s\n\n\n\n\n\n\n\n\n\n\n\n\nJul 26, 2023\n\n\nDaniel Falbel\n\n\n\n\n\n\n\n\nAutoscaling self-hosted runners for GH Actions\n\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2023\n\n\nDaniel Falbel\n\n\n\n\n\n\n\n\nMac Mini as a headless server\n\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nDaniel Falbel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]