{
  "articles": [
    {
      "path": "about.html",
      "title": "About this blog",
      "description": "Some additional details about the blog",
      "author": [],
      "contents": "\nWelcome to this blog. After quite some time daily writing R code, I decided to put this blog to keep up some notes and help me remembering details of implementations.\nHere I’ll mostly talk about torch for R and the problems faced while implementing it.\nHopefully it’s helpful for others!\n\n\n\n",
      "last_modified": "2021-05-27T15:03:28-03:00"
    },
    {
      "path": "index.html",
      "title": "",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-05-27T15:03:28-03:00"
    },
    {
      "path": "loss-for-classification-in-torch.html",
      "title": "Classification losses in torch",
      "description": "In this article we describe the many ways one can compute loss functions\nin binary classification problems when using torch.\n",
      "author": [
        {
          "name": "Daniel Falbel",
          "url": "https://github.com/dfalbel"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nChoosing the correct loss function is an important step in machine learning problems. In general it depends on the problem you are trying to solve and how you described this problem in mathematical terms (or probabilistic terms). The mathematical definition should lead you to a loss function that you want to minimize.\nHowever, because computers live in a world where there’s finite precision, the same loss function can have multiple implementations that account for different numerical stability problems. The way you encode your data can also influence the implementation you chose.\ntorch won’t hide this from you and leaves to the user the choice of which implementation to use and this can be quite confusing. Let’s take a look at multiple ways to compute the cross entropy in torch, for both binary and multi-class classification problems.\nBinary cross-entropy\nThe binary cross-entropy or logloss is defined as:\n\\[\nL(\\hat{y},y) = - \\left[ y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}) \\right]\n\\]\nWhere \\(\\hat{y}\\) is the an estimate for \\(P(y=1)\\).\nThis exact same formula is implemented in torch in the torch::nn_bce_loss(). So given y and y_hat we can compute the binary cross entropy with:\n\n\ny <- torch_tensor(c(1,1,1,0,0,0))\ny_hat <- torch_tensor(c(0.7, 0.8, 0.9, 0.1, 0.2, 0.3))\nloss <- torch::nn_bce_loss()\nloss(y_hat, y)\n\n\ntorch_tensor\n0.228393\n[ CPUFloatType{} ]\n\nTurns out that, since it’s hard to keep constraints like \\(\\hat{y} \\in (0,1)\\) when doing optimization it’s common practice to write \\(\\hat{y} = \\sigma(z)\\) where \\(\\sigma\\) is the sigmoid function - defined by \\(\\sigma(x) = \\frac{1}{1+ e^{-x}}\\). There are other reasons for that, but intuitively this makes sense because this function takes any real number and puts it into the \\((0,1)\\) interval. However, exponentiating numbers in the real scale can quickly become a problem of numerical stability.\nTo help fix this we can write the binary cross-entropy in terms of the logits \\(z\\) :\n\\[\nL(z,y) = - \\left[ y \\log( \\frac{1}{1+ e^{-z}}) + (1-y)\\log(1-\\frac{1}{1+ e^{-z}}) \\right]\n\\]\nAnd here we have a sum of logarithms of exponential values and we can use the LogSumExp trick to get a more numerical stable version. And this is what torch::nn_bce_with_logits_loss() implements. You can see that if use the torch::nn_bce_with_logits_loss() with the logits, computed by using the inverse of the sigmoid function we get the same results as using the binary cross entropy.\n\n\n# inverse of the sigmoid function\nz <- torch_log(y_hat/(1-y_hat)) \nloss <- torch::nn_bce_with_logits_loss()\nloss(z, y)\n\n\ntorch_tensor\n0.228393\n[ CPUFloatType{} ]\n\nNegative likelihood loss\nThere’s yet another way to obtain this same value in torch, but now it’s not related to numerical stability but to how you prefer encoding your data. You might want to encode your data in the following setting.\nFirst instead of encoding your labels as a degenerated distribution containing 0s and 1s only we will encode them as indexes. Indexes in torch are represented with the torch_long() and they start at 1, so we do:\n\n\ny2 <- (y + 1)$to(dtype = torch_long())\ny2\n\n\ntorch_tensor\n 2\n 2\n 2\n 1\n 1\n 1\n[ CPULongType{6} ]\n\nAlso, instead of representing the probabilities a single vector containing the the \\(P(y=2)\\) we will now represent them by a matrix with 2 columns. THe first columns is \\(P(y=1)\\) and the second is \\(P(y=2)\\). Note that this has the exact same information as before because given one column you can easily find the other by doing \\(1-p\\) .\n\n\ny_hat2 <- torch_stack(list(1-y_hat, y_hat), dim = 2)\ny_hat2\n\n\ntorch_tensor\n 0.3000  0.7000\n 0.2000  0.8000\n 0.1000  0.9000\n 0.9000  0.1000\n 0.8000  0.2000\n 0.7000  0.3000\n[ CPUFloatType{6,2} ]\n\nWe can then use the Negative Likelihood loss to compute the same quantity with:\n\n\nloss <- torch::nn_nll_loss()\nloss(torch_log(y_hat2), y2)\n\n\ntorch_tensor\n0.228393\n[ CPUFloatType{} ]\n\nNote that the NLL takes log probabilities instead of logits or probabilities. This blogpost by Sebastian Raschka has a nice explanation on how the cross entropy relates to the NLL.\nFim\nWe conclude this post with a table that tries to summarize the different loss functions for classification in torch. Even though we didn’t talk about multiclass-classification here, this table will also describe it:\nName\nInput\nTarget\nnn_bce_loss()\nVector of probabilities\nVector of probabilities. usually a degenerated one containing only 0s and 1s.\nnn_bce_with_logits_loss()\nVector of logits\nVector of probabilities. usually a degenerated one containing only 0s and 1s.\nnn_nll_loss()\nMatrix of log-probabilities for each class\nIndexes for each class. Remember: indexes start at 1 and should have torch_long() dtype.\nnn_cross_entropy_loss()\nMatrix of logits for each class.\nIndexes for each class. Remember: indexes start at 1 and should have torch_long() dtype.\n\n\n\n",
      "last_modified": "2021-05-27T15:03:30-03:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
