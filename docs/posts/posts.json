[
  {
    "path": "posts/2021-05-27-classification-losses-in-torch/",
    "title": "Classification losses in torch",
    "description": "In this article we describe the many ways one can compute loss functions\nin binary classification problems when using torch.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2021-05-27",
    "categories": [],
    "contents": "\nChoosing the correct loss function is an important step in machine learning problems. In general it depends on the problem you are trying to solve and how you described this problem in mathematical terms (or probabilistic terms). The mathematical definition should lead you to a loss function that you want to minimize.\nHowever, because computers live in a world where there’s finite precision, the same loss function can have multiple implementations that account for different numerical stability problems. The way you encode your data can also influence the implementation you chose.\ntorch won’t hide this from you and leaves to the user the choice of which implementation to use and this can be quite confusing. Let’s take a look at multiple ways to compute the cross entropy in torch, for both binary and multi-class classification problems.\nBinary cross-entropy\nThe binary cross-entropy or logloss is defined as:\n\\[\nL(\\hat{y},y) = - \\left[ y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}) \\right]\n\\]\nWhere \\(\\hat{y}\\) is the an estimate for \\(P(y=1)\\).\nThis exact same formula is implemented in torch in the torch::nn_bce_loss(). So given y and y_hat we can compute the binary cross entropy with:\n\n\ny <- torch_tensor(c(1,1,1,0,0,0))\ny_hat <- torch_tensor(c(0.7, 0.8, 0.9, 0.1, 0.2, 0.3))\nloss <- torch::nn_bce_loss()\nloss(y_hat, y)\n\n\ntorch_tensor\n0.228393\n[ CPUFloatType{} ]\n\nTurns out that, since it’s hard to keep constraints like \\(\\hat{y} \\in (0,1)\\) when doing optimization it’s common practice to write \\(\\hat{y} = \\sigma(z)\\) where \\(\\sigma\\) is the sigmoid function - defined by \\(\\sigma(x) = \\frac{1}{1+ e^{-x}}\\). There are other reasons for that, but intuitively this makes sense because this function takes any real number and puts it into the \\((0,1)\\) interval. However, exponentiating numbers in the real scale can quickly become a problem of numerical stability.\nTo help fix this we can write the binary cross-entropy in terms of the logits \\(z\\) :\n\\[\nL(z,y) = - \\left[ y \\log( \\frac{1}{1+ e^{-z}}) + (1-y)\\log(1-\\frac{1}{1+ e^{-z}}) \\right]\n\\]\nAnd here we have a sum of logarithms of exponential values and we can use the LogSumExp trick to get a more numerical stable version. And this is what torch::nn_bce_with_logits_loss() implements. You can see that if use the torch::nn_bce_with_logits_loss() with the logits, computed by using the inverse of the sigmoid function we get the same results as using the binary cross entropy.\n\n\n# inverse of the sigmoid function\nz <- torch_log(y_hat/(1-y_hat)) \nloss <- torch::nn_bce_with_logits_loss()\nloss(z, y)\n\n\ntorch_tensor\n0.228393\n[ CPUFloatType{} ]\n\nNegative likelihood loss\nThere’s yet another way to obtain this same value in torch, but now it’s not related to numerical stability but to how you prefer encoding your data. You might want to encode your data in the following setting.\nFirst instead of encoding your labels as a degenerated distribution containing 0s and 1s only we will encode them as indexes. Indexes in torch are represented with the torch_long() and they start at 1, so we do:\n\n\ny2 <- (y + 1)$to(dtype = torch_long())\ny2\n\n\ntorch_tensor\n 2\n 2\n 2\n 1\n 1\n 1\n[ CPULongType{6} ]\n\nAlso, instead of representing the probabilities a single vector containing the the \\(P(y=2)\\) we will now represent them by a matrix with 2 columns. THe first columns is \\(P(y=1)\\) and the second is \\(P(y=2)\\). Note that this has the exact same information as before because given one column you can easily find the other by doing \\(1-p\\) .\n\n\ny_hat2 <- torch_stack(list(1-y_hat, y_hat), dim = 2)\ny_hat2\n\n\ntorch_tensor\n 0.3000  0.7000\n 0.2000  0.8000\n 0.1000  0.9000\n 0.9000  0.1000\n 0.8000  0.2000\n 0.7000  0.3000\n[ CPUFloatType{6,2} ]\n\nWe can then use the Negative Likelihood loss to compute the same quantity with:\n\n\nloss <- torch::nn_nll_loss()\nloss(torch_log(y_hat2), y2)\n\n\ntorch_tensor\n0.228393\n[ CPUFloatType{} ]\n\nNote that the NLL takes log probabilities instead of logits or probabilities. This blogpost by Sebastian Raschka has a nice explanation on how the cross entropy relates to the NLL.\nFim\nWe conclude this post with a table that tries to summarize the different loss functions for classification in torch. Even though we didn’t talk about multiclass-classification here, this table will also describe it:\nName\nInput\nTarget\nnn_bce_loss()\nVector of probabilities\nVector of probabilities. usually a degenerated one containing only 0s and 1s.\nnn_bce_with_logits_loss()\nVector of logits\nVector of probabilities. usually a degenerated one containing only 0s and 1s.\nnn_nll_loss()\nMatrix of log-probabilities for each class\nIndexes for each class. Remember: indexes start at 1 and should have torch_long() dtype.\nnn_cross_entropy_loss()\nMatrix of logits for each class.\nIndexes for each class. Remember: indexes start at 1 and should have torch_long() dtype.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-27T15:03:57-03:00",
    "input_file": "classification-losses-in-torch.knit.md"
  },
  {
    "path": "posts/2021-04-15-cosine-embedding-loss-in-torch/",
    "title": "Cosine Embedding loss in torch",
    "description": "This is a short post showing the coside embedding loss in action with torch.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2021-04-15",
    "categories": [],
    "contents": "\nThe cosine embedding loss in torch is given by:\n\\[\\text{loss}(x, y) =\n        \\begin{cases}\n        1 - \\cos(x_1, x_2), & \\text{if } y = 1 \\\\\n        \\max(0, \\cos(x_1, x_2) - \\text{margin}), & \\text{if } y = -1\n        \\end{cases}\\]\nThe idea is to give it a pair of vectors and and a response value \\(1\\) or \\(-1\\) depending on if they belong to the same group or not.\nFirst let’s generate small dataset of 20 observations, each one being a length 100 vector. We also sample for each obs if it belongs to group ‘a’ or group ‘b’:\n\n\nlibrary(torch)\nlibrary(ggplot2) # going to use for plotting\nx <- torch_randn(20, 100)\ny <- sample(c(\"a\", \"b\"), replace = TRUE, size = x$shape[1])\n\n\n\nNext, we crate a torch dataset that will do the following everytime we ask for a new item \\(i\\):\nTake the observation corresponding to the item \\(i\\) in the dataset we created previously.\nWith \\(prob=0.5\\) select an observation from the same group to be it’s pair, otherwise select an observation from a different group.\nReturn both selected observations and the objective value \\(-1\\) of they are obs are not from the same group and \\(1\\) if they are.\n\n\ndata <- dataset(\n  initialize = function(x, y, rate = 0.5) {\n    self$x <- x\n    self$y <- y\n    self$rate <- rate\n  },\n  .getitem = function(i) {\n    \n    lab <- self$y[i]\n    if (self$rate < runif(1)) {\n      i_ <- sample(which(self$y == lab), 1)\n      obj <- 1\n    } else {\n      i_ <- sample(which(self$y != lab), 1)\n      obj <- -1\n    }\n    \n    list(x = self$x[i,], x_ = self$x[i_,], obj = obj)\n  },\n  .length = function() {\n    x$shape[1]\n  }\n)\n\n# Initialize the dataset and dataloaders\nd <- data(x, y)\ndl <- dataloader(d, batch_size = 5)\n\n\n\nThe model we are going to define is a dimension reduction model. It will take the observation space from 100 dimensions to only 2: \\(\\mathbb{R}_{100} \\Rightarrow \\mathbb{R}_2\\). It does that via linear model.\n\n\nmodel <- nn_linear(100, 2)\n\n\n\nWe create a plotting utility to plot observations in the model space.\n\n\nmake_plot <- function(model, x, y) {\n  with_no_grad({\n    as <- as.data.frame(as.matrix(model(x[y == \"a\", ])))\n    as$class <- \"a\"\n    bs <- as.data.frame(as.matrix(model(x[y == \"b\", ])))\n    bs$class <- \"b\"  \n  })\n  \n  ggplot(rbind(as, bs), aes(x = V1, y = V2, color = class)) +\n    geom_point() +\n    xlim(-2, 2) +\n    ylim(-2, 2)\n}\n\n\n\nWe now fit this model for 100 epochs saving intermediary plots:\n\n\nopt <- optim_adam(model$parameters)\ncriterion <- nn_cosine_embedding_loss()\n\nplots <- list()\nfor (epoch in 1:100) {\n  coro::loop(for (b in dl) {\n    r <- model(b$x)\n    r_ <- model(b$x_)\n    \n    opt$zero_grad()\n    loss <- criterion(r, r_, b$obj)\n    loss$backward()\n    opt$step()\n    \n  })\n  plots[[length(plots) + 1]] <- make_plot(model, x, y)\n}\n\n\n\nAnd we can finally observe how the observations evolve in the model space during training:\n\n\ngifski::save_gif(delay = 0.1, lapply(seq_along(plots), function(x) {\n  p <- plots[[x]] +\n    ggtitle(label = sprintf(\"Epoch %02d\", x))\n  print(p)\n}), \"animation.gif\")\n\n\n[1] \"/Users/dfalbel/Documents/dfalbel.github.io/_posts/2021-04-15-cosine-embedding-loss-in-torch/animation.gif\"\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-15T12:59:20-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-31-gh-actions-self-hosted/",
    "title": "Self-created custom runners with Github Actions",
    "description": "In this article we describe the approach we are using to \ncreate and manage custom GPU accelerated Windows instances\nto run CI workflows in the torch project.",
    "author": [
      {
        "name": "Daniel Falbel",
        "url": "https://github.com/dfalbel"
      }
    ],
    "date": "2021-03-31",
    "categories": [],
    "contents": "\nGitHub Actions is a tool to automate CI/CD workflows. In the torch for R project besides the hosted runners offered by GitHub that allow us to test torch in all major platforms, we need to test our code in GPU accelerated systems.\nGitHub Actions doesn’t offer GPU accelerated hosted runners but allows one to easily add self-hosted runners to a repository.\nAdding a self-hosted runner to a repository is a relatively task when the machine that is going to run the workflows is persistent, ie. the machine is turned on most of the time expect for a few restarts or etc.\nHowever GPU’s are quite expensive and most cloud offerings charge ~$250 monthly for a 24h - 7/7 machine equipped with an NVIDIA Tesla K80 GPU. THis is not nice specially because the instance would be idle for a large fraction of this period.\nThe ideal solution for us would be to spin up a GPU equipped ephemeral instance every time we would like to run our workflow and turn it off once it’s finished. If we were to do it on Linux there are a few posts that help configuring ephemeral self-hosted runners for GitHub Actions. Specifically, this series by Stephen O’Brien is great. Docker and nvidia-docker helps a lot here!\nOn Windows though, there’s no support for nvidia-docker, so we had to build another custom solution.\nOverview\nOverview diagramOur workflow will have 3 chained jobs that will do the following:\nCreate virtual machines on Google Cloud Platform, install the Github Actions runner and register them trough the GH Actions API.\nThe machines that were created in step .1 will take on the CI jobs. Eg running R CMD CHECK. This can possibly be a matrix of jobs.\nDelete the VM instances and unregister the runners from HJ Actions.\n1. Creating runners\nFirst step to create the runners was to create a machine image with basic software pre-installed - this is similar to what GH Actions has for the hosted runners here.\nTo do this we first created a VM instance in Google Cloud and manually installed the CUDA toolkit versions, CUDNN libraries, CMake, Visual Studio Community (for the vsc compiler) and Git.\nOnce you have manually installed everything you can create the machine image. I used the web UI of google cloud console, mostly following this tutorial. It’s basically selecting the source VM (the one we already installed the basic software) and giving a name to the machine image. In my case it was “cuda-v1” or something like that.\nNow that we have the machine image we can create the ‘start-runners’ job in the GitHub workflow .yaml file.\nThe full job description looks like this and we describe each step in code comments:\nstart-runner:\n    runs-on: ubuntu-18.04\n    steps:\n    \n    # Installs the gcloud SDK and set's up the creadentials\n    # We are are going to use the SDK to create the VM instances.\n    - name: Set up Cloud SDK\n      uses: google-github-actions/setup-gcloud@master\n      with:\n        project_id: ${{ secrets.GCP_PROJECT_ID }}\n        service_account_key: ${{ secrets.GCP_APPLICATION_CREDENTIALS }}\n        export_default_credentials: true\n    \n    # Creates the token used to register a new self-hosted runner via\n    # the GH actions API.\n    - name: Create registration token\n      uses: actions/github-script@v3\n      id: token\n      with:\n        github-token: ${{secrets.GH_TOKEN}}\n        result-encoding: string\n        script: |\n          const result = await github.actions.createRegistrationTokenForRepo({\n            owner: 'mlverse',\n            repo: 'torch'\n          });\n          console.log(result)\n          return result.data.token\n          \n    # In this step we create an startup script that is run once\n    # when the VM instance is created.\n    # The startup script we create here is responsible for downloading\n    # and installing the actions runner system and registering it.\n    # This script will create a runner with a label 'run-${{ github.run_number}}'.\n    # We are going to use this information to select the runner that will be used\n    # for runnig the tests in step 2.\n    # We used Power Shell (.ps1) script but it could also be a `.bat` file.\n    - name: Create the startup script\n      run: |\n        touch s.ps\n        echo \"mkdir C:\\actions-runner; cd C:\\actions-runner\" >> s.ps\n        echo \"Invoke-WebRequest -Uri https://github.com/actions/runner/releases/download/v2.277.1/actions-runner-win-x64-2.277.1.zip -OutFile actions-runner-win-x64-2.277.1.zip\" >> s.ps\n        echo 'Add-Type -AssemblyName System.IO.Compression.FileSystem ; [System.IO.Compression.ZipFile]::ExtractToDirectory(\"$PWD/actions-runner-win-x64-2.277.1.zip\", \"$PWD\")' >> s.ps\n        echo \"./config.cmd --url https://github.com/mlverse/torch --token ${{ steps.token.outputs.result }} --name 'runner-${{ github.run_number }}' --labels 'run-${{ github.run_number}}' --unattended\" >> s.ps\n        echo 'Start-Process -FilePath \"./run.cmd\"' >> s.ps\n        \n    # Now we can create the VM instance on GCP using the `gcloud` SDK\n    # that we already installed.\n    # The `source-machine-image` parameter describes which machine\n    # will be used for the VM you create. You should pass the same\n    # name as you used when you created the machine image.\n    # The `metadata-from-file` parameter is telling gcloud to copy\n    # the startup scrip we just created and ru it when the machine starts.\n    # Other parameters are related to the resources you want your machine\n    # to have. In our case we used a n1-standard-8 machine equipped with a\n    # NVIDIA Tesla K80 GPU.\n    - name: Create instance\n      run: | \n        gcloud components install beta --quiet\n        gcloud beta compute --project=rstudio-cloudml instances create runner-${{github.run_number}} \\\n          --zone=us-central1-a \\\n          --machine-type=n1-standard-8 \\\n          --accelerator=type=nvidia-tesla-k80,count=1 \\\n          --source-machine-image cuda-v8 \\\n          --network-interface network=default \\\n          --metadata-from-file windows-startup-script-ps1=s.ps\n    # It might take some time for the startup script to run and we want\n    # to wait for a couple of minutes before starting the next job, because it\n    # requires that the self-hosted runner is already registered.\n    - name: Wait for runner registration\n      run: |\n        sleep 2m 30s\n2. Running the tests\nThis job is where we actually define what we want to run in the VM we just created and registered.\nWe just want to run R CMD CHECK in our package to make sure everything works on NVIDIA GPU’s on Windows.\nThe workflow description is pretty much the same as the one we use in the hosted runner, however we condition the run-on so this job runs in the runner we just created.\nThe job looks like the chunk below. Comments in the important parts.\nwindows-gpu:\n    # this is important so this job only runs after a successful run of the \n    # `start-runner` job\n    needs: ['start-runner']\n    strategy:\n      fail-fast: false\n      matrix:\n        r_version: [\"release\"]\n        cuda: [\"11.1\"]\n\n    # Here we tell GH actions to run this job in a runner labeled \n    # 'run-${{ github.run_number}}' which is exactly the runner we\n    # created and registered in step 1.\n    # Everything below here is identical to the job we run on the\n    # GH Actions hosted runner.\n    runs-on: [self-hosted, windows, 'run-${{ github.run_number}}']\n    \n    name: 'Windows | CUDA: ${{ matrix.cuda }}'\n    timeout-minutes: 120\n    env:\n      R_REMOTES_NO_ERRORS_FROM_WARNINGS: true\n      INSTALL_TORCH: 1\n      TORCH_LOG: 2\n      TORCH_TEST: 1\n      TORCH_INSTALL: 1\n      CUDA: ${{ matrix.cuda }}\n      \n    steps:\n      - uses: actions/checkout@v2\n      - uses: r-lib/actions/setup-r@master\n        with:\n          r-version: ${{ matrix.r_version }}\n      - uses: r-lib/actions/setup-pandoc@master\n      - name: Install dependencies\n        run: | \n          Rscript -e \"options(repos=structure(c(CRAN='https://cloud.r-project.org/'))); install.packages(c('remotes', 'rcmdcheck', 'fs'))\" -e \"remotes::install_deps(dependencies = TRUE)\"\n      - name: Build lantern and get libtorch\n        if: contains( github.event.pull_request.labels.*.name, 'lantern')\n        run: | \n          Rscript tools/buildlantern.R\n      - name: Check\n        run:  |\n          withr::with_makevars(list(MAKEFLAGS=\"-j8\"), {\n            rcmdcheck::rcmdcheck(args = c(\"--no-multiarch\", \"--no-manual\"), error_on = \"error\", check_dir = \"check\")\n          })\n        shell: Rscript {0}\n3. Clean up resources\nThis step runs after the test job runs and is responsible for correctly cleaning up the resources, ie. deleting the VM instance on google cloud and removing the runner in GH Actions UI.\nWe do this using the following job:\ndelete-runner:\n    # This if statement is important so this jobs runs even if the testing fails.\n    # By default, GH Actions would skip it if the testing failed.\n    if: ${{ success() || failure() || cancelled() }}\n    # This defines that this job will run after the testing job and after the job\n    # that creates the instances.\n    needs: ['windows-gpu', 'start-runner'] \n    runs-on: ubuntu-18.04\n    steps:\n    # Again we setup gcloud\n    - name: Set up Cloud SDK\n      uses: google-github-actions/setup-gcloud@master\n      with:\n        project_id: ${{ secrets.GCP_PROJECT_ID }}\n        service_account_key: ${{ secrets.GCP_APPLICATION_CREDENTIALS }}\n        export_default_credentials: true\n    # This used the the gcloud CLI to delete the instance with name that we\n    # gave in the startup script.\n    - name: Delete runner instance\n      run: |\n        gcloud compute --project=rstudio-cloudml instances delete runner-${{github.run_number}} --zone=us-central1-a\n    # This removes the runner from the GH self-hosted registry.\n    # This is not strictly necessary as GH actions seems to delete unreachable\n    # runners after some time but looks like good practice.\n    - name: Delete runner from GH\n      uses: actions/github-script@v3\n      id: token\n      with:\n        github-token: ${{secrets.GH_TOKEN}}\n        result-encoding: string\n        script: |\n          const runners = await github.actions.listSelfHostedRunnersForRepo({\n            owner: 'mlverse',\n            repo: 'torch'\n          });\n          \n          const runner_id = runners.data.runners.filter((runner) => {\n            return runner.name === 'runner-${{ github.run_number }}';\n          })[0].id\n          \n          await github.actions.deleteSelfHostedRunnerFromRepo({\n            owner: 'mlverse',\n            repo: 'torch',\n            runner_id: runner_id\n          });\nFim\nIn this post we described our approach for self-hosted runners with GPU’s on Windows. It’s not trivial and this post is far from being detailed. Feel free to reach out if you have other questions or if we can help with anything. The full workflow is here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-12T17:23:48-03:00",
    "input_file": {}
  }
]
