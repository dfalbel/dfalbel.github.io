---
layout: post
title: 'Validação Adversária'
date : 2016-06-12
tags: [modelagem, randomForest]
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[Esse post](http://fastml.com/adversarial-validation-part-one/) do [FastML](http://fastml.com/) fez uma 
análise bem interessante sobre como são as bases de validação/avaliação de torneios de Machine Learning 
como os que aparecem no [Kaggle](https://www.kaggle.com/) ou o do [Numerai](https://numer.ai/). Ele comenta
como em algumas competições, a base de validação (base em que são avaliadas as predições para o cálculo
do seu score) possuem comportamento diferente da base utilizada para treino. Nesse post, que é a primeira parte
da análise do Zygmund, ele mostra o exemplo de uma competição do Santander, que aconteceu no Kaggle em
que as duas bases de treino e teste possuem o mesmo comportamento. 

Na [parte 2 do post](http://fastml.com/adversarial-validation-part-two/), ele mostra que a base de treino do 
[Numerai](https://numer.ai/), aquela da qual eles deixam a variável `target` disponível é bem diferente do banco de teste, que eles chamam de *tournament dataset*. Até o próprio Numerai, recomendou a leitura deste post no twitter.

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Interesting idea on adversarial validation with Numerai data on <a href="https://twitter.com/fastml">@fastml</a> <a href="https://t.co/dqb0WupiMH">https://t.co/dqb0WupiMH</a></p>&mdash; Numerai (@numerai) <a href="https://twitter.com/numerai/status/740709465964478464">June 9, 2016</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

Resolvi replicar o experimento dele para verificar o resultado encontrado por ele. Utilizei a mesma idéia
proposta pelo FastMl: Empilhar a base do torneio e a base de treino e tentar criar um modelo para prever se
uma observação é da base de treino. Se o acerto do modelo for perto de 50%, quer dizer que as bases são muito parecidas. Se o modelo conseguir acertar de qual base de dados a observação é proveniente, significa que os
bancos de dados possuem características muito diferentes.

# Leitura das bases

Usei as bases que estão disponíveis no Numerai neste [link](https://numer.ai/). Também as deixei dispoíveis 
no [repositório do blog](https://github.com/dfalbel/dfalbel.github.io/tree/master/data/numerai-datasets).

```{r}
library(dplyr)
train_data <- read.csv("../data/numerai-datasets/numerai_training_data.csv")
test_data <- read.csv("../data/numerai-datasets/numerai_tournament_data.csv")
```

Empilhando as duas e criando a resposta.

```{r}
data <- bind_rows(
  train_data %>% select(-target) %>% mutate(TRAIN = TRUE),
  test_data %>% select(-t_id) %>% mutate(TRAIN = FALSE)
)
data$TRAIN <- as.factor(data$TRAIN)
```

# Visualização

Para visualizar as possíveis diferenças entre a base de treino e de torneio, utilizamos componentes principais para reduzir a 
dimensionalidade. Com isso, das 21 variáveis que possíamos inicialmente, obtivemos 2, que podem ser facilmente ser representadas
em um gráfico de dispersão.

```{r}
pca <- princomp(data %>% select(-TRAIN))
pca_df <- pca$scores %>%
  data.frame()
pca_df$TRAIN <- data$TRAIN
```

```{r}
library(ggplot2)
pca_df %>% 
  arrange(runif(nrow(.))) %>%
  ggplot(aes(Comp.1, Comp.2)) + geom_point(aes(color = TRAIN), size = 0.2, alpha = 0.3)
```

Note que com essas duas dimensões não é perceptível a diferença entre os dois bancos de dados.

# Ajustando o modelo

Aqui usei um modelo de random forest por meio do pacote `caret`.

```{r}
library(randomForest)
modelo <- randomForest(TRAIN ~ ., data = data, ntree = 100, mtry = sqrt(21))
modelo
```

Na tabela acima, as linhas repesentam os valores verdadeiros e as colunas as categorias previstas pelo
modelo de random forest. Note que das 96.320 observações da base de treino, o modelo classifica apenas
589 (menos de 1%) como base de teste. 

```{r}
library(ROCR)
pred <- prediction(predict(modelo, type = "prob")[,2], data$TRAIN)
as.numeric(performance(pred , "auc")@y.values)
```

Veja também que o AUC (área sobre a curva ROC) foi de `r as.numeric(performance(pred , "auc")@y.values) %>% round(2)`, muito próxima da relatada no post do FastML. Se a base de testes fosse realmente uma
amostra aleatória da base de treino, esse número deveria ser próximo de 0.5.

Enfim, esse resultado é importante pois, se as duas bases possuem comportamentos diferentes, é difícil
saber qual o score que você teria no final do torneio, apenas avaliando o seu erro na base de treino.
A recomendação do FastMl é validar o seu modelo nestas observações da base de treini que o modelo de random forest classificou errôneamente como base de teste, desta forma você teria uma estimativa mais
precisa do erro no final do torneio.